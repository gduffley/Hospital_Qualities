{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to explore if there are relationships between local economies and hospital quality.\n",
    "\n",
    "The goal of this notebook is to explore if per capita personal income is related to hospital quality. Income data comes from https://apps.bea.gov/api/data/, and the hospital quality data comes from https://data.medicare.gov. The final product should be two SQL tables, \"hospitals\" and \"incomes\". There should be a complete many to one join between the \"hospitals\" and \"incomes\" tables. The joined table should result in there being economic data associated with each hospital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a019b6658282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msodapy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSocrata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;31m# file of API keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keys'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "import requests\n",
    "import json\n",
    "from sodapy import Socrata\n",
    "import keys # file of API keys\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import scipy as sp\n",
    "\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create hospital dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# API call to the Data.Medicare.Gov Hospital General Information dataset\n",
    "# The medicare data is available through Socrata Open Data API, which can be easily queried using sodapy\n",
    "client = Socrata(\"data.medicare.gov\", None)\n",
    "# \"xubh-q36u\" is the hospital general information dataset id\n",
    "results = client.get(\"xubh-q36u\", limit = 10000)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "hr_df = pd.DataFrame.from_records(results)\n",
    "hr_df.describe()\n",
    "\n",
    "# Because we are trying to map locations and values to hospitals we will focus on geography data. \n",
    "# The income data is done by county, we are going to use the county name and state data to map hospitals to \n",
    "# local economic metrics\n",
    "# State is in 'state' using two letter abbr. and the county name is ALL CAPS in county_name. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create per capita income dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Call to get average income per capita per county from SEA data\n",
    "key = keys.sea_api_key\n",
    "econ_addr = \"https://apps.bea.gov/api/data/\"\n",
    "\n",
    "# 'CAINC1' is the personal income table, and linecode 3 is the per capita incomes\n",
    "parameters = {\n",
    "    \"UserID\" : key,\n",
    "    \"DATASETNAME\" : 'Regional',\n",
    "    \"RESULTFORMAT\" : 'JSON',\n",
    "    \"METHOD\" : \"GETDATA\",\n",
    "    'GEOFIPS': 'COUNTY',\n",
    "    'TABLENAME': 'CAINC1',\n",
    "    'YEAR': '2018',\n",
    "    'LINECODE': '3'\n",
    "}\n",
    "\n",
    "response = requests.get(econ_addr, parameters)\n",
    "inc_json = response.json()\n",
    "\n",
    "# print(inc_json.keys()\n",
    "# out: ['BEAAPI']\n",
    "# print(inc_json['BEAAPI'].keys())\n",
    "# out: ['Request', 'Results'])\n",
    "# print(inc_json['BEAAPI']['Results'].keys()\n",
    "# out: ['Statistic', 'UnitOfMeasure', 'PublicTable', 'UTCProductionTime', 'NoteRef', 'Dimensions', 'Data', 'Notes']\n",
    "\n",
    "# once down to the flat data layer, then turn into a df\n",
    "inc_data = inc_json['BEAAPI']['Results']['Data']\n",
    "inc_df = pd.DataFrame(inc_data)\n",
    "inc_df.describe()\n",
    "\n",
    "\n",
    "# GeoFips = unique identifier for each region. Use as foreign key in hospitals table.\n",
    "# GeoName is string of \"county name, state\", each is unique. These values will be what is mapped to \n",
    "# the hospital data to established which GeoFip should be added to the hospital table row.\n",
    "# NoteREf: notes about the data. The notes have to do with how county definitons have changed\n",
    "# over time, but not relevant given we are working with modern hospital data that should \n",
    "# be encodded using modern county definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the county and state name into seperate cols to be compared with the hospital\n",
    "\n",
    "# using right split because the state comes after the last comma\n",
    "county_state_split = inc_df.GeoName.str.rsplit(\",\", n=1, expand=True)\n",
    "inc_df['county_name'] = county_state_split[0]\n",
    "inc_df['state'] = county_state_split[1]\n",
    "\n",
    "\n",
    "# need to make the county name all upper to match the econ data\n",
    "inc_df['county_name'] = inc_df['county_name'].str.upper().str.strip()\n",
    "\n",
    "# The hospital data lists the county of hospitals in independent cities as:\n",
    "# X CITY (e.g. ALEXANDRIA CITY)\n",
    "# where the SEA data lists as:\n",
    "# \"X (INDEPENDENT CITY)\" (e.g. ALEXANDRIA (INDEPENDENT CITY))\n",
    "# so convert to same string format\n",
    "ind_city = (inc_df.county_name.str.contains(\"(INDEPENDENT CITY)\", regex=False))\n",
    "inc_df.loc[ind_city, 'county_name'] = inc_df.loc[ind_city, 'county_name'].str.replace(\n",
    "    \"(INDEPENDENT CITY)\", \"CITY\", regex=False)\n",
    "\n",
    "# Remove the potentially redundant city added in if the independent city's name is X CITY\n",
    "# e.g. Carson City\n",
    "city_city = (inc_df.county_name.str.contains(\"CITY CITY\", regex=False))\n",
    "inc_df.loc[city_city, 'county_name'] = inc_df.loc[city_city, 'county_name'].str.replace(\n",
    "    \"CITY \", \"\", n=1, regex=False)         \n",
    "            \n",
    "# Because there are some very small independent cities, they are often grouped with their\n",
    "# nearby county for census purposes. The income data has the listed county for these places\n",
    "# as \"COUNTY + CITY\" or \"COUNTY, CITY + CITY\" \n",
    "# So these need to be split out into individual cols, and for the every col\n",
    "# After the first one, there needs to be a  \"CITY\" appended to match the \n",
    "# syntax the hospital data uses for independent cities\n",
    "# As a result, the economic data table won't just have a single county name value, but actually \n",
    "# multiple county/city names in the counties_split_cols \n",
    "counties_split_out = inc_df.county_name.str.split(r\" \\+ |, \", expand = True)\n",
    "counties_split_cols = [\"county_name_{}\".format(i) for i in range(counties_split_out.shape[1])]\n",
    "inc_df[counties_split_cols] = counties_split_out\n",
    "for col in counties_split_cols[1:]:\n",
    "    inc_df[col] = inc_df[col] + \" CITY\"\n",
    "\n",
    "# drop the county name col now that it has been split out\n",
    "inc_df = inc_df.drop('county_name', 1)\n",
    "\n",
    "# need to remove the \"*\" that imply there exists a note on the state abbr.\n",
    "inc_df['state'] = inc_df['state'].str.strip().str.replace(\"*\", \"\")\n",
    "\n",
    "# Make the data values numeric. The income data uses commas in the numbers\n",
    "# that need to be removed\n",
    "inc_df['DataValue'] = pd.to_numeric(inc_df['DataValue'].str.replace(\",\", \"\"), errors = 'coerce')\n",
    "\n",
    "# drop the 25 cols that have nans. These are county designations that \n",
    "# have been used in the past, but not in 2018\n",
    "inc_df = inc_df.dropna(subset = ['DataValue'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual modifications of the dataset\n",
    "\n",
    "# The income dataset does not have income data for US territories, so drop out the hospital data\n",
    "territories = ['VI', 'AS', 'MP', 'GU', 'PR']\n",
    "hr_df = hr_df.loc[~hr_df.state.isin(territories)]\n",
    "\n",
    "# Fixing typos in the hospital names.\n",
    "# TODO: remove manual typo correction and use fuzzywuzzy\n",
    "typos = {\"NORTHWEST ARTIC BOROUGH\" : \"NORTHWEST ARCTIC BOROUGH\", \"NORTH SLOPE BOROUH\" : \"NORTH SLOPE BOROUGH\",\n",
    "         'JEFFRSON DAVIS' : 'JEFFERSON DAVIS','E. BATON ROUGE' : 'EAST BATON ROUGE', 'ST. JOHN BAPTIST': \n",
    "         'ST. JOHN THE BAPTIST', 'SITKA BOROUGH' : 'SITKA CITY AND BOROUGH', 'DONA ANA': 'DOÃ‘A ANA',\n",
    "         'LAKE OF  WOODS' : 'LAKE OF THE WOODS', 'YELLOW MEDCINE': 'YELLOW MEDICINE', 'SCOTT BLUFF': 'SCOTTS BLUFF',\n",
    "         'NORTHUMBERLND' : 'NORTHUMBERLAND', 'LAPAZ': 'LA PAZ'}\n",
    "for key in typos:\n",
    "    hr_df.loc[hr_df.county_name == key, \"county_name\"] = typos[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first step is to find the exact matches between\n",
    "# the state and county_name from the hospital df and the state and first counties_split_cols in the income df\n",
    "# df is going to be the final hosptial df that we turn into a SQL table. The goal is it to be the same as\n",
    "# the original table with a new col named GeoFips that corresponds to the correct GeoFip in the income \n",
    "# data table\n",
    "\n",
    "# By doing an inner merge on, only going to get rows where both match togehter. By only selecting for hr_df cols\n",
    "# and \"GeoFips\", df is just the hr_df where there is an exact match and the correct GeoFip for the hospital\n",
    "df = pd.merge(hr_df, inc_df,  how='inner', left_on=['county_name','state'],\n",
    "              right_on=[counties_split_cols[0],'state'], suffixes = [None, None])[list(hr_df) + ['GeoFips']]\n",
    "\n",
    "\n",
    "# Remove the rows from hr_df that are now in df\n",
    "hr_df = hr_df.loc[~hr_df.provider_id.isin(df.provider_id)]\n",
    "print(hr_df.shape)\n",
    "print(df.shape)\n",
    "# We have perfect matches for all but 102 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to check for matches on other counties_split_cols. Could do with inner merge like for the first call,\n",
    "# but this is an alternate approach.\n",
    "# First, left join the remaining unmatched rows of the hr_dr and the inc_df on state only\n",
    "alt_matches_df = pd.merge(hr_df, inc_df,  how='left', on='state')\n",
    "# Then find rows where hr_df matches any of the counties_split_cols\n",
    "alt_matches_df = alt_matches_df.loc[alt_matches_df[counties_split_cols].isin(alt_matches_df.county_name).any(1)]\n",
    "# Take those rows and append to the df with only the columns from the df\n",
    "df = pd.concat([df, alt_matches_df[list(df)]])\n",
    "hr_df = hr_df.loc[~hr_df.provider_id.isin(df.provider_id)]\n",
    "# Update the hr_df with the missing\n",
    "print(alt_matches_df.shape)\n",
    "print(hr_df.shape)\n",
    "print(df.shape)\n",
    "# we added 19 more rows to the df, and have 83 unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DC just uses a different name for the county name between the two datasets, but both consider it\n",
    "# a single county, so just join by state only\n",
    "# Get the DC GeoFip\n",
    "dc_ind = inc_df.loc[inc_df.state == \"DC\", 'GeoFips'].values[0]\n",
    "hr_df.loc[hr_df.state == \"DC\"]\n",
    "# Assign to the hr_df DC rows\n",
    "temp = hr_df.loc[hr_df.state == \"DC\"]\n",
    "# append new rows to df and remove from hr_df\n",
    "df = pd.concat([df, temp])\n",
    "df.loc[df.state == \"DC\", \"GeoFips\"] = dc_ind\n",
    "hr_df = hr_df.loc[~hr_df.provider_id.isin(df.provider_id)]\n",
    "print(temp.shape)\n",
    "print(hr_df.shape)\n",
    "print(df.shape)\n",
    "# we added 9 more rows to the df, and have 74 unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the exact same thing as the original merge, but remove spaces and apostrophes from both county_name\n",
    "# cols to enable an inexact match\n",
    "# TODO: Do this step with fuzzywuzzy\n",
    "\n",
    "hr_df['county_name_ns'] = hr_df.county_name.str.replace(\" \", \"\", regex = False).str.replace(\"\\'\", \"\", regex = False)\n",
    "inc_df[counties_split_cols[0] + \"_na\"] = inc_df[counties_split_cols[0]].str.replace(\" \", \"\", regex = False).str.replace(\"\\'\", \"\", regex = False)\n",
    "temp = pd.merge(hr_df, inc_df,  how='inner', left_on=['county_name_ns','state'],\n",
    "              right_on=[counties_split_cols[0]  + \"_na\",'state'], suffixes = [None, None])[list(df)]\n",
    "\n",
    "# remove the hr_df['county_name_ns'] to keep the columns the same as df before appending\n",
    "temp = temp.drop('county_name_ns', 1)\n",
    "# append new rows to df and remove from hr_df\n",
    "df = pd.concat([df, temp])\n",
    "hr_df = hr_df.loc[~hr_df.provider_id.isin(df.provider_id)]\n",
    "print(temp.shape)\n",
    "print(hr_df.shape)\n",
    "print(df.shape)\n",
    "# we added 55 more rows to the df, and have 19 unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alaskan county names are written out more formally in the inc_data than in the hr_df\n",
    "# So do a substring search to see if the shorter name from hr_df is in the longer name \n",
    "# of the inc_df\n",
    "# TODO: Do this step with fuzzywuzzy\n",
    "\n",
    "# Do a left merge on \"state\" for Alaska only. This creates a many to many match for each row in the hr_dr\n",
    "# From there, can check if the hr_dr county name is a substring of the counties_split_cols[0]. Keep\n",
    "# only the rows where it is true\n",
    "temp = pd.merge(hr_df.loc[hr_df.state == \"AK\"], inc_df,  how='left', on='state')\n",
    "# use a lambda function to see if col is substring of other col\n",
    "temp = temp.loc[temp.apply(lambda x: x.county_name in x[counties_split_cols[0]], axis=1)]\n",
    "\n",
    "# append new rows to df and remove from hr_df\n",
    "df = pd.concat([df, temp[list(df)]])\n",
    "hr_df = hr_df.loc[~hr_df.provider_id.isin(df.provider_id)]\n",
    "print(temp.shape)\n",
    "print(hr_df.shape)\n",
    "print(df.shape)\n",
    "# we added 19 more rows to the df, and have 0 unmatched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dummy sql server in memory\n",
    "conn = sqlite3.connect(\":memory:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert the geocode col which is a dictionary to a string for sql encoding\n",
    "df['geocoded_column'] = df['geocoded_column'].astype(str)\n",
    "df.to_sql('hospitals', con=conn)\n",
    "\n",
    "# 'SELECT FirstName, LastName    -- list out: first names and surnames\n",
    "#   FROM Patients\n",
    "#   WHERE NOT EXISTS (    -- patients who do not meet criteria\n",
    "#     SELECT * FROM Appointments -- get all columns from selection\n",
    "#         WHERE Patients.PatientID = Appointments.PatientID -- appointment is for the patient\n",
    "#           AND Appointments.AppointmentDate > DATEADD(year, -1, GETDATE())    -- appointments in last year\n",
    "#           AND Appointments.AppointmentType = 1)    -- appointments that are AWVs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_df.to_sql('income', con=conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# left join of hospitals and income on GeoFips filtered for hospitals\n",
    "# where the average income is greater than 50k \n",
    "query = \"\"\"SELECT * \n",
    "           FROM hospitals \n",
    "           LEFT JOIN income ON hospitals.GeoFips = income.GeoFips\n",
    "           WHERE DATAVALUE > 75000\"\"\"\n",
    "\n",
    "n_df = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT * \n",
    "           FROM hospitals \n",
    "           LEFT JOIN income ON hospitals.GeoFips = income.GeoFips\"\"\"\n",
    "\n",
    "n_df = pd.read_sql_query(query, conn)\n",
    "n_df['hospital_overall_rating'] = pd.to_numeric(n_df['hospital_overall_rating'], errors= 'coerce')\n",
    "\n",
    "n_df = n_df.dropna(subset = ['DataValue', 'hospital_overall_rating'])\n",
    "sns.boxplot(y = 'DataValue', x = 'hospital_overall_rating', data = n_df)\n",
    "plt.figure()\n",
    "sns.distplot(n_df.DataValue)\n",
    "plt.figure()\n",
    "sns.distplot(n_df.hospital_overall_rating)\n",
    "\n",
    "#There is a weak, but significant correlation between per capita income and overall hospital quality\n",
    "sp.stats.spearmanr(n_df.DataValue, n_df.hospital_overall_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
